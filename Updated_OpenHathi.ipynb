{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaitaanshDixit/Research_Paper/blob/main/Updated_OpenHathi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmDhVhaZ_5aF"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01Jz94ryAZz7"
      },
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pV8GuXX7AgHF"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flcVHPBbAl-B"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "print(sys.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TCkDwQCAn36"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LV2OoCLAwpb"
      },
      "outputs": [],
      "source": [
        "pip show transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y086n4k5A16n"
      },
      "outputs": [],
      "source": [
        "pip install --force-reinstall --no-deps torch==2.0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u98m3x0GA5aH"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3sImpBwLN9U"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bteQCSPLV-M"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip cache purge  # Clears any old cached versions\n",
        "\n",
        "# Install the correct version for Colab's CUDA (11.8)\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_2zgjiZCWf1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.version.cuda)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlhTn3FvGpbI"
      },
      "outputs": [],
      "source": [
        "pip install bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zF07bXC2MbJN"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iimmppNNGufp"
      },
      "outputs": [],
      "source": [
        "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
        "import torch\n",
        "\n",
        "\"\"\"\n",
        "# Load the tokenizer and model\n",
        "tokenizer = LlamaTokenizer.from_pretrained('sarvamai/OpenHathi-7B-Hi-v0.1-Base')\n",
        "model = LlamaForCausalLM.from_pretrained('sarvamai/OpenHathi-7B-Hi-v0.1-Base', torch_dtype=torch.bfloat16)\n",
        "\n",
        "# Test with a sample input\n",
        "text = \"Hello, this is a test.\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "print(outputs)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SORr7dQG7_O"
      },
      "outputs": [],
      "source": [
        "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
        "import torch\n",
        "\n",
        "\"\"\"\n",
        "# Load the tokenizer and model\n",
        "tokenizer =LlamaTokenizer.from_pretrained('sarvamai/OpenHathi-7B-Hi-v0.1-Base')\n",
        "model = LlamaForCausalLM.from_pretrained('sarvamai/OpenHathi-7B-Hi-v0.1-Base', torch_dtype=torch.bfloat16)\n",
        "\n",
        "# prompt = \"आप एक संक्षिप्ती प्रणाली हैं और आपको दिए गए पैराग्राफ को 150 शब्दों में संक्षेपित करने का काम दिया गया है।\"  # Your prompt in Hindi\n",
        "# original_news_article_text =  \"\"\" #आपदा प्रबंधन मंत्री प्रो चंद्रशेखर ने आरोप लगाया है कि बिहार से केंद्र सरकार में शामिल मंत्री बाढ़ को लेकर सिर्फ़ 'राजनीति' कर रहे हैं. हालांकि प्रधानमंत्री नरेंद्र मोदी बाढ़ प्रभावित राज्यों के मुख्यमंत्रियों को हर संभव मदद का आश्वासन दे चुके हैं. बिहार के मुख्यमंत्री नीतीश कुमार ने बुधवार को मोदी से मुलाक़ात भी की थी. दूसरी तरफ पर्यावरणविद् हिमांशु ठक्कर बाढ़ जैसे हालत के लिए सभी सरकारों को कठघरे में खड़ा करते हैं. उनका कहना है कि विकास की योजनाएं बनाते वक्त सरकारें पर्यावरण के बारे में नहीं सोचती हैं. बीबीसी से बातचीत में उन्होंने कहा, \"बाढ़ तो आनी ही है. लेकिन ये विभीषिका में न तब्दील हो जाए, इसके बारे में विचार किए जाने की ज़रूरत है.\" बिहार के आपदा प्रबंधन मंत्री प्रो चंद्रशेखर ने बीबीसी से बातचीत में कहा कि बिहार एक साथ सूखे और बाढ़ से तबाही का दंश झेल रहा है. उन्होंने कहा कि बिहार में इस बार मानसून की बारिश 14 फ़ीसदी कम हुई है और राज्य का 70 फ़ीसदी इलाका सूखे की चपेट में है. लेकिन कोसी, सोन और गंगा के दियारा क्षेत्र के बहुत से ज़िले बाढ़ से प्रभावित हैं. प्रो चंद्रशेखर के मुताबिक़ बाढ़ की वजह से बिहार की 56 लाख आबादी प्रभावित है. उन्होंने कहा कि बिहार सरकार पीड़ितों तक मदद पहुंचाने के लिए हर संभव कोशिश में जुटी है. प्रो चंद्रशेखर ने कहा, \"बहुत दुख से कहना पड़ता है वो कि एक महीने से तबाही के मंज़र का तमाशा देख रहे हैं. केंद्र सरकार में शामिल आधा दर्जन बिहार के मंत्री राजनीति कर रहे हैं. उन्हें कौन रोकता है सहायता करने से. हमने तो गुहार भी लगा दी है.\" प्रो चंद्रशेखर का कहना है कि बाढ़ की समस्या बिहार के लिए आयातित है. ये समस्या मध्य प्रदेश, झारखंड और नेपाल की देन है. बाढ़ पीड़ितों तक मदद देर से पहुंचने के सवाल पर वो कहते हैं कि राज्य सरकार हर तरह की सहायता पहुंचाने में सक्षम है लेकिन 56 लाख की आबादी प्रभावित तो मशीनरी को पहुंचने में वक्त लग सकता है. बाढ़ पूर्व तैयारी न होने के आरोपों को भी प्रो चंद्रशेखर खारिज करते हैं. उन्होंने कहा, \"बाढ़ पूर्व तैयारी के तहत आज से पांच महीने पहले सारे जिलों को आबादी के हिसाब से पैसे दिए गए थे. हमारे विभाग के स्तर पर आवंटन में 12 घंटे का भी विलंब नहीं होता है.\" उन्होंने कहा कि बिहार सरकार बाढ़ के जोखिम को कम करने के लिए 15 साल के प्रोजेक्ट पर काम कर रही है.\"\"\"\n",
        "# input_text = f\"{prompt} {original_news_article_text}\"  # Combine prompt and original text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpMDtzMSNeTl"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch, re\n",
        "\n",
        "\"\"\"\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('sarvamai/OpenHathi-7B-Hi-v0.1-Base')\n",
        "\n",
        "# Load the model with 4-bit quantization (optional for efficiency)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    'sarvamai/OpenHathi-7B-Hi-v0.1-Base',\n",
        "    load_in_4bit=True,  # Optional for efficiency (4-bit quantization)\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqZetL9JN-Ef"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
        "import torch\n",
        "\n",
        "\"\"\"\n",
        "# Load the tokenizer and model\n",
        "tokenizer = LlamaTokenizer.from_pretrained('sarvamai/OpenHathi-7B-Hi-v0.1-Base')\n",
        "model = LlamaForCausalLM.from_pretrained('sarvamai/OpenHathi-7B-Hi-v0.1-Base', torch_dtype=torch.bfloat16)\n",
        "\"\"\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=6.0)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"sarvamai/OpenHathi-7B-Hi-v0.1-Base\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",         # splits layers across GPU/CPU\n",
        "    torch_dtype=torch.float16  # fastest on T4\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sarvamai/OpenHathi-7B-Hi-v0.1-Base\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_OtYve7OkLz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "def enforce_sentence_limit(summary, min_sentences=5, max_sentences=7):\n",
        "    # Split the summary into sentences using punctuation marks\n",
        "    sentences = re.split(r'(?<=[।!?.])\\s+', summary.strip())\n",
        "    # Filter out any empty strings\n",
        "    sentences = [s for s in sentences if s]\n",
        "    # If the number of sentences exceeds the maximum, truncate\n",
        "    if len(sentences) > max_sentences:\n",
        "        sentences = sentences[:max_sentences]\n",
        "    # If the number of sentences is less than the minimum, consider regenerating\n",
        "    elif len(sentences) < min_sentences:\n",
        "        # Optionally, you can implement a loop to regenerate the summary\n",
        "        pass\n",
        "    # Join the sentences back into a single string\n",
        "    return ' '.join(sentences)\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "def generate_summary(text, prompt=\"\"):\n",
        "    full_text = f\"{prompt}\\n{text}\" if prompt else text\n",
        "\n",
        "    # Tokenize the input and send it to the device where the model is\n",
        "    inputs = tokenizer(full_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    # Send input tensors to the device where the model is located\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    summary_ids = model.generate(\n",
        "        # Access 'input_ids' directly from the inputs dictionary\n",
        "        inputs['input_ids'],\n",
        "        max_length=512,\n",
        "        max_new_tokens=200,\n",
        "        min_length=100,\n",
        "        length_penalty=1.2,\n",
        "        num_beams=4,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        no_repeat_ngram_size=3,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    raw_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True).strip()\n",
        "    final_summary = enforce_sentence_limit(raw_summary)\n",
        "    return final_summary\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "def generate_summary(text):\n",
        "    #prompt = \"आप एक संक्षिप्ती प्रणाली हैं और आपको दिए गए पैराग्राफ को 150 शब्दों में संक्षेपित करने का काम दिया गया है।\"  # Your prompt in Hindi\n",
        "    #input_text = f\"{prompt} {text}\"  # Combine prompt and original text\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\",truncation=True, max_length=512)\n",
        "    summary_ids = model.generate(inputs.input_ids, max_new_tokens=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    #final_summary = enforce_sentence_limit(summary)\n",
        "    return summary\n",
        "\"\"\"\n",
        "\n",
        "def enforce_sentence_limit(text, min_sentences=5, max_sentences=7):\n",
        "    sentences = re.split(r'(?<=[।!?])\\s+', text.strip())\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    return ' '.join(sentences[:max_sentences])\n",
        "\n",
        "\n",
        "def generate_summary(article_text):\n",
        "    prompt = (\n",
        "        \"### निर्देश:\\n\"\n",
        "        \"आप एक पेशेवर हिंदी पत्रकार हैं। नीचे दिए गए लेख को ध्यान से पढ़ें और उसे अपने शब्दों में पूरी तरह से दोबारा लिखें, \"\n",
        "    \"ताकि यह एक सरल, संक्षिप्त और स्पष्ट समाचार सारांश बन जाए।\\n\"\n",
        "    \"सारांश अधिकतम 150 शब्दों में हो, अंग्रेज़ी शब्दों का प्रयोग न करें, और उसमें केवल मुख्य बिंदु शामिल हों। \"\n",
        "    \"अनावश्यक विवरण या मूल वाक्यों को दोहराने से बचें।\\n\\n\"\n",
        "        \"### इनपुट:\\n\"\n",
        "        f\"{article_text}\\n\\n\"\n",
        "        \"### आउटपुट:\\n\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        summary_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            do_sample=True,             # Enable sampling\n",
        "            top_k=50,                   # Keep top 50 tokens to sample from\n",
        "            top_p=0.95,                 # Nucleus sampling\n",
        "            temperature=0.9,            # Increase creativity\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    out = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return out.split(\"### आउटपुट:\")[-1].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-sY-PG_O4My"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "\n",
        "prompt = \"\"\" आप एक अनुभवी भाषा विशेषज्ञ हैं। आपका कार्य निम्नलिखित समाचार लेख को अधिकतम 300 शब्दों में सरल, संक्षिप्त और स्पष्ट हिंदी में सारांशित करना है। कृपया दोहराव से बचें, केवल महत्वपूर्ण बिंदुओं को शामिल करें, और अंग्रेजी का प्रयोग न करें। \"\"\"\n",
        "\n",
        "\n",
        "original_news_article_text = \"\"\" कृत्रिम बुद्धिमत्ता आज के आधुनिक युग में एक क्रांतिकारी तकनीक बन चुकी है, जिसने मानव जीवन के हर क्षेत्र को प्रभावित किया है। स्वास्थ्य सेवा, शिक्षा, यातायात, बैंकिंग, मनोरंजन, और यहां तक कि कृषि जैसे क्षेत्रों में भी इसने कार्यप्रणाली को अधिक कुशल, तेज़ और सटीक बनाया है। अस्पतालों में आधारित चित्र संसाधन से रोगों की पहले से पहचान हो रही है, जिससे इलाज समय पर हो पा रहा है। शिक्षा के क्षेत्र में व्यक्तिगत अध्ययन प्रणाली और भाषा अनुवाद की सुविधाएँ छात्रों की सीखने की गति को बढ़ा रही हैं। वित्तीय क्षेत्र में धोखाधड़ी की पहचान और ग्राहक सहायता सेवाओं को आसान बना रहे हैं। कृषि में फसल की निगरानी और सिंचाई प्रबंधन में आधारित संवेदक बड़े बदलाव ला रहे हैं। यातायात में बुद्धिमान मार्ग प्रणाली और स्वचालित वाहन भविष्य को और अधिक सुरक्षित बना सकते हैं। हालांकि, इसके उपयोग से कुछ जोखिम भी हैं, जैसे बेरोजगारी में वृद्धि, निर्णयों में पक्षपात, और आँकड़ा गोपनीयता से जुड़े मुद्दे। अगर सही दिशा में उपयोग न हो, तो यह तकनीक समाज के लिए खतरनाक भी साबित हो सकती है। इसके अलावा, सैन्य क्षेत्र में इसका प्रयोग घातक हथियारों के निर्माण में किया जा रहा है, जो मानवीय नैतिकता पर प्रश्नचिन्ह खड़ा करता है। अतः जरूरी है कि इसके विकास के साथ-साथ इसके उपयोग को नियंत्रित करने के लिए कड़े नियम और नैतिक दिशानिर्देश भी हों। सरकारों, वैज्ञानिकों और नीति-निर्माताओं को मिलकर इस तकनीक को मानवता के हित में दिशा देनी चाहिए। \"\"\"\n",
        "#input_text = f\"{prompt} {original_news_article_text}\"  # Combine prompt and original text\n",
        "\n",
        "summary = generate_summary(original_news_article_text)\n",
        "\n",
        "\"\"\"\n",
        "def remove_english_words(text):\n",
        "    # Remove words containing A-Z or a-z\n",
        "    return re.sub(r'\\b[a-zA-Z]+\\b', '', text)\n",
        "\n",
        "clean_summary = remove_english_words(summary)\n",
        "\"\"\"\n",
        "\n",
        "print(\"Generated Summary:\", summary)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMmTI4HCg8uMmP5rXoErRJf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}